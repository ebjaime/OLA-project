{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import bisect"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non stationary Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment():\n",
    "\n",
    "  def __init__(self, n_arms, probabilities, bids, average_number_of_clicks, average_cum_daily_cost,noise_clicks,noise_cost):\n",
    "    self.n_arms = n_arms                                            # number of prices\n",
    "    self.probabilities = probabilities                              # conversion rates for every price/arm\n",
    "    self.bids = bids                                                # bids\n",
    "    self.average_number_of_clicks = average_number_of_clicks        # curve of average number of clicks (y = f(bids))\n",
    "    self.average_cum_daily_cost = average_cum_daily_cost            # curve of cumulative daily cost (y = g(bids))\n",
    "    self.noise_clicks = noise_clicks                                # gaussian noise for the average number of clicks sampling\n",
    "    self.noise_cost = noise_cost                                    # gaussian noise for the cumulative daily cost sampling\n",
    "\n",
    "  # daily rewards\n",
    "  def bidding_round(self, pulled_bid):\n",
    "    clicks = int(np.random.normal(self.average_number_of_clicks(self.bids[pulled_bid]),self.noise_clicks))        # number of people that click on the ad\n",
    "    reward_click = clicks if clicks >= 0 else 0\n",
    "    costs = np.random.normal(self.average_cum_daily_cost(self.bids[pulled_bid]),self.noise_cost)                  # cumulative daily cost\n",
    "    reward_cost = costs if costs > 0 else 0\n",
    "\n",
    "    return reward_click, reward_cost\n",
    "\n",
    "  def pricing_round(self, pulled_price):\n",
    "    reward_price = np.random.binomial(1,self.probabilities[pulled_price])                         # number of people that buy once they clicked\n",
    "    return reward_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonStationnary(Environment):\n",
    "  def __init__(self, n_arms, probabilities, bids, average_number_of_clicks, average_cum_daily_cost,noise_clicks,noise_cost,\n",
    "              time_abrupt_change_1, time_abrupt_change_2):\n",
    "    super().__init__(n_arms, probabilities, bids, average_number_of_clicks, average_cum_daily_cost,noise_clicks,noise_cost)\n",
    "    self.time_abrupt_change_1 = time_abrupt_change_1\n",
    "    self.time_abrupt_change_2 = time_abrupt_change_2\n",
    "    \n",
    "  def pricing_round(self, pulled_price):\n",
    "    print(\"Error: Non-stationnary environment needs the time to know the conversion rates!\")\n",
    "    return float(\"NaN\")\n",
    "    \n",
    "  def pricing_round(self, pulled_price, time):\n",
    "    if time < self.time_abrupt_change_1:\n",
    "      reward_price = np.random.binomial(1,self.probabilities[0,pulled_price])\n",
    "    elif time < self.time_abrupt_change_2:\n",
    "      reward_price = np.random.binomial(1,self.probabilities[1,pulled_price])\n",
    "    else:\n",
    "      reward_price = np.random.binomial(1,self.probabilities[2,pulled_price])\n",
    "    return reward_price\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pricing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner:\n",
    "  def __init__(self,n_arms):\n",
    "    self.n_arms = n_arms\n",
    "    self.t = 0                                              # current round value\n",
    "    self.rewards_per_arm = x = [[] for i in range(n_arms)]  # value of collected rewards for each round and for each arm\n",
    "    self.collected_rewards = np.array([])                   # values of collected rewards for each round\n",
    "\n",
    "  # function that updates the observation's list once the reward is returned by the environment\n",
    "  def update_observations(self, pulled_arm, reward):\n",
    "    self.rewards_per_arm[pulled_arm].append((reward, self.t))\n",
    "    self.collected_rewards = np.append(self.collected_rewards,reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TS_Pricing_Learner(Learner): # Thompson-Sampling (reward: number of conversions; actual_reward:  price*conversion_rate)\n",
    "  def __init__(self,n_arms,prices):\n",
    "    super().__init__(n_arms)                    # number of prices\n",
    "    self.beta_parameters = np.ones((n_arms,2))  # parameters of beta distributions\n",
    "    self.prices = prices                        # prices (array)\n",
    "\n",
    "    #self.empirical_means = np.zeros(n_arms)\n",
    "\n",
    "  def pull_arm(self):\n",
    "    if(self.t < self.n_arms):\n",
    "      return self.t\n",
    "    sampled = np.random.beta(self.beta_parameters[:,0],self.beta_parameters[:,1])*self.prices\n",
    "    idx = np.argmax(sampled)\n",
    "    #return idx, sampled[idx]\n",
    "    return idx\n",
    "\n",
    "  # update parameters each time a reward in {0,1} is observed\n",
    "  def update(self,pulled_arm, reward):\n",
    "    self.t += 1\n",
    "    self.update_observations(pulled_arm,reward*self.prices[pulled_arm])\n",
    "    self.beta_parameters[pulled_arm,0] = self.beta_parameters[pulled_arm,0] + reward\n",
    "    self.beta_parameters[pulled_arm,1] = self.beta_parameters[pulled_arm,1] + 1 - reward\n",
    "\n",
    "    #self.empirical_means[pulled_arm] = (self.empirical_means[pulled_arm]*(len(self.rewards_per_arm[pulled_arm]) - 1) + reward*self.prices[pulled_arm] ) / len(self.rewards_per_arm[pulled_arm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCB_Pricing_Learner(Learner): # UCB1 (reward: number of conversions; actual_reward:  price*conversion_rate)\n",
    "  def __init__(self,n_arms,prices):\n",
    "    super().__init__(n_arms)                              # number of arms/prices\n",
    "    self.empirical_means = np.zeros(n_arms)               # mean reward for each arm (conversion rate)\n",
    "    self.confidence = np.zeros(n_arms)                    # confidence bound for each arm\n",
    "    self.prices = prices                                  # prices (array)\n",
    "\n",
    "  def pull_arm(self):\n",
    "    if(self.t < self.n_arms):\n",
    "      return self.t\n",
    "    upper_bound = self.empirical_means + self.confidence\n",
    "    pulled_arm = np.random.choice(np.where(upper_bound == upper_bound.max())[0])\n",
    "    #return pulled_arm, upper_bound[pulled_arm]\n",
    "    #return pulled_arm, self.empirical_means[pulled_arm]\n",
    "    return pulled_arm\n",
    "\n",
    "  def update(self, pulled_arm, reward):\n",
    "    self.t += 1\n",
    "    self.update_observations(pulled_arm, reward*self.prices[pulled_arm])\n",
    "    self.empirical_means[pulled_arm] = (self.empirical_means[pulled_arm]*(len(self.rewards_per_arm[pulled_arm]) - 1) + reward*self.prices[pulled_arm] ) / len(self.rewards_per_arm[pulled_arm])\n",
    "    for a in range(self.n_arms):\n",
    "      self.confidence[a] = self.prices[a]*np.sqrt(2*np.log(self.t)/len(self.rewards_per_arm[a])) if len(self.rewards_per_arm[a]) > 0 else 1e3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SW_UCB_Pricing_Learner(Learner): # UCB1 (reward: number of conversions; actual_reward:  price*conversion_rate)\n",
    "  def __init__(self,n_arms,prices, tau=np.sqrt(365)):\n",
    "    super().__init__(n_arms)                              # number of arms/prices\n",
    "    self.empirical_means = np.zeros(n_arms)               # mean reward for each arm (conversion rate)\n",
    "    self.confidence = np.zeros(n_arms)                    # confidence bound for each arm\n",
    "    self.prices = prices                                  # prices (array)\n",
    "    self.tau = tau                                        # parameter for the confidence bound\n",
    "\n",
    "  def count_elements_in_range(self, data, time, tau=0):\n",
    "    start_time = time - tau if time - tau > 0 else 0\n",
    "    start_index = bisect.bisect_left(data, (0, start_time)) \n",
    "    end_index = bisect.bisect_right(data, (0, time))\n",
    "    count = end_index - start_index\n",
    "    return count\n",
    "\n",
    "  def pull_arm(self):\n",
    "    if(self.t < self.n_arms):\n",
    "      return self.t\n",
    "    \n",
    "    for a in range(self.n_arms):\n",
    "      if self.count_elements_in_range(self.rewards_per_arm[a], self.t - 1, self.tau) == 0:\n",
    "        return a\n",
    "\n",
    "    upper_bound = self.empirical_means + self.confidence\n",
    "    pulled_arm = np.random.choice(np.where(upper_bound == upper_bound.max())[0])\n",
    "    return pulled_arm\n",
    "\n",
    "  def update(self, pulled_arm, reward):\n",
    "    self.t += 1\n",
    "    self.update_observations(pulled_arm, reward*self.prices[pulled_arm])\n",
    "    self.empirical_means[pulled_arm] = (self.empirical_means[pulled_arm]*(len(self.rewards_per_arm[pulled_arm][0]) - 1 - self.tau) + reward*self.prices[pulled_arm] ) / len(self.rewards_per_arm[pulled_arm][0])\n",
    "    for a in range(self.n_arms):\n",
    "      self.confidence[a] = self.prices[a]*np.sqrt(2*np.log(self.t)/self.count_elements_in_range(data=self.rewards_per_arm[a], time=(self.t - 1), tau=self.tau)) if self.count_elements_in_range(data=self.rewards_per_arm[a], time=(self.t - 1), tau=self.tau) > 0 else 1e3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generic\n",
    "T = 365                                # horizon of experiment\n",
    "n_experiments = 100                    # since the reward functions are stochastic, to better visualize the results and remove the noise we do multiple experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pricing\n",
    "n_prices = 5\n",
    "prices = [5,6,7,8,9]\n",
    "p = np.array([[0.15,0.1,0.1,0.35,0.1],\n",
    "              [0.5,0.2,0.1,0.1,0.1],\n",
    "              [0.1,0.1,0.1,0.2,0.5]])             # bernoulli distributions for the reward functions\n",
    "\n",
    "time_abrupt_change_1 = 100\n",
    "time_abrupt_change_2 = 200\n",
    "opt_rate_0 = p[0, np.argmax(p[0,:]*prices)]                 # optimal arm\n",
    "opt_rate_1 = p[1, np.argmax(p[1,:]*prices)] \n",
    "opt_rate_2 = p[2, np.argmax(p[2,:]*prices)] \n",
    "print(\"Pricing (optimal price):\")\n",
    "print(\"Before change:\")\n",
    "print(\" idx: \" + str(np.argmax(p[0,:]*prices)) + \n",
    "      \"  price: \" + str(prices[np.argmax(p[0,:]*prices)]) + \n",
    "      \"  rate: \" + str(opt_rate_0) + \n",
    "      \"  price*rate: \" + str(opt_rate_0*prices[np.argmax(p[0,:]*prices)]))\n",
    "print(\"After 1 change:\")\n",
    "print(\" idx: \" + str(np.argmax(p[1,:]*prices)) + \n",
    "      \"  price: \" + str(prices[np.argmax(p[1,:]*prices)]) + \n",
    "      \"  rate: \" + str(opt_rate_1) + \n",
    "      \"  price*rate: \" + str(opt_rate_1*prices[np.argmax(p[1,:]*prices)]))\n",
    "print(\"After 2 changes:\")\n",
    "print(\" idx: \" + str(np.argmax(p[2,:]*prices)) + \n",
    "      \"  price: \" + str(prices[np.argmax(p[2,:]*prices)]) + \n",
    "      \"  rate: \" + str(opt_rate_2) + \n",
    "      \"  price*rate: \" + str(opt_rate_2*prices[np.argmax(p[2,:]*prices)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# advertising\n",
    "n_bids = 100\n",
    "min_bid = 0.0\n",
    "max_bid = 1.0\n",
    "bids = np.linspace(min_bid, max_bid, n_bids)\n",
    "sigma_clicks = 3\n",
    "sigma_costs = 3\n",
    "def clicks(x):\n",
    "  return 100 * (1.0 - np.exp(-4*x+3*x**3))\n",
    "def costs(x):\n",
    "  return 70 * (1.0 - np.exp(-7*x))\n",
    "opt_bid_0 = bids[np.argmax(opt_rate_0*prices[np.argmax(p[0,:]*prices)]*clicks(bids)-costs(bids))]\n",
    "opt_bid_1 = bids[np.argmax(opt_rate_1*prices[np.argmax(p[1,:]*prices)]*clicks(bids)-costs(bids))]\n",
    "opt_bid_2 = bids[np.argmax(opt_rate_2*prices[np.argmax(p[2,:]*prices)]*clicks(bids)-costs(bids))]\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(bids,clicks(bids),'blue',bids,costs(bids),'orange')\n",
    "ax.legend([\"Number of clicks\", \"Cumulative Costs\"])\n",
    "ax.axvline(opt_bid_0,c='red')\n",
    "ax.axvline(opt_bid_1,c='green')\n",
    "ax.axvline(opt_bid_2,c='purple')\n",
    "# print(\"Advertising (optimal bid):\")\n",
    "# print(\"idx: \" + str(np.argmax(opt_rate*prices[np.argmax(p*prices)]*clicks(bids)-costs(bids))) + \"  bid: \" + str(opt_bid) + \"  clicks-costs: \" + str(clicks(opt_bid)-costs(opt_bid)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Known advertising"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are looking for the best $\\tau$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_experiments = 1  #CHECK FOR DIFFERENT VALUES OF n_experiments\n",
    "taus = np.linspace(1,365,20) # FIND THE BEST TAU\n",
    "\n",
    "sw_ucb_rewards_per_experiment_per_tau = np.empty((len(taus), n_experiments))\n",
    "\n",
    "\n",
    "for tau in tqdm(taus):\n",
    "    sw_ucb_rewards_per_experiment = np.array([])\n",
    "    for e in tqdm(range(n_experiments)):\n",
    "        env = NonStationnary(n_arms = n_prices,\n",
    "                       probabilities = p,\n",
    "                       bids = bids,\n",
    "                       average_number_of_clicks = clicks,\n",
    "                       average_cum_daily_cost = costs,\n",
    "                       noise_clicks = sigma_clicks,\n",
    "                       noise_cost = sigma_costs,\n",
    "                       time_abrupt_change_1 = time_abrupt_change_1,\n",
    "                       time_abrupt_change_2 = time_abrupt_change_2)\n",
    "        learner = SW_UCB_Pricing_Learner(n_prices, prices, tau)\n",
    "\n",
    "        sw_ucb_collected_costs = np.array([])\n",
    "        sw_ucb_daily_pricing_reward = np.array([])\n",
    "        \n",
    "        for t in range(T):\n",
    "            pulled_bid = np.argmax(clicks(bids)-costs(bids))\n",
    "            reward_click, reward_cost = env.bidding_round(pulled_bid)\n",
    "            sw_ucb_collected_costs = np.append(sw_ucb_collected_costs, reward_cost)\n",
    "\n",
    "            for k in range(reward_click):\n",
    "                pulled_price = learner.pull_arm()\n",
    "                reward_price = env.pricing_round(pulled_price, t)\n",
    "                learner.update(pulled_price, reward_price)\n",
    "            sw_ucb_daily_pricing_reward = np.append(sw_ucb_daily_pricing_reward,(reward_click>0)*np.sum(learner.collected_rewards[-reward_click:]))\n",
    "        sw_ucb_rewards_per_experiment = np.append(sw_ucb_rewards_per_experiment, sw_ucb_daily_pricing_reward-sw_ucb_collected_costs)\n",
    "    sw_ucb_rewards_per_experiment_per_tau = np.append(sw_ucb_rewards_per_experiment_per_tau,[sw_ucb_rewards_per_experiment])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_0 = [opt_rate_0*prices[np.argmax(p[0,:]*prices)]*clicks(opt_bid_0) - costs(opt_bid_0) \n",
    "         for i in range(time_abrupt_change_1)]\n",
    "opt_1 = [opt_rate_1*prices[np.argmax(p[1,:]*prices)]*clicks(opt_bid_1) - costs(opt_bid_1) \n",
    "         for i in range(time_abrupt_change_2-time_abrupt_change_1)]\n",
    "opt_2 = [opt_rate_2*prices[np.argmax(p[2,:]*prices)]*clicks(opt_bid_2) - costs(opt_bid_2) \n",
    "         for i in range(T-time_abrupt_change_2)]\n",
    "opt = np.array(opt_0 + opt_1 + opt_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0)\n",
    "plt.ylabel(\"Regret\")\n",
    "plt.xlabel(\"t\")\n",
    "min = -1\n",
    "min_ind = -1\n",
    "for i in range(0, len(taus)):\n",
    "    cumsum = np.cumsum(np.mean(opt - sw_ucb_rewards_per_experiment_per_tau[i], axis = 0))\n",
    "    if min > cumsum[-1] or min==-1:\n",
    "        min = cumsum[-1]\n",
    "        min_ind = i\n",
    "    plt.plot(cumsum)\n",
    "plt.legend([\"SW UCB tau = \" + str(taus[i]) for i in range(0, len(taus))], bbox_to_anchor=(1.05, 1), loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SW UCB tau = \" + str(taus[min_ind]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiments\n",
    "n_experiments = 100\n",
    "ts_rewards_per_experiment = []\n",
    "ucb_rewards_per_experiment = []\n",
    "sw_ucb_rewards_per_experiment = []\n",
    "\n",
    "for e in tqdm(range(0,n_experiments)):  # cycle on experiments\n",
    "  env = NonStationnary(n_arms = n_prices,\n",
    "                       probabilities = p,\n",
    "                       bids = bids,\n",
    "                       average_number_of_clicks = clicks,\n",
    "                       average_cum_daily_cost = costs,\n",
    "                       noise_clicks = sigma_clicks,\n",
    "                       noise_cost = sigma_costs,\n",
    "                       time_abrupt_change_1 = time_abrupt_change_1,\n",
    "                       time_abrupt_change_2 = time_abrupt_change_2)\n",
    "  ts_learner = TS_Pricing_Learner(n_arms = n_prices,\n",
    "                                       prices = prices)\n",
    "  learner = UCB_Pricing_Learner(n_arms = n_prices,\n",
    "                                    prices = prices)\n",
    "  sw_learner = SW_UCB_Pricing_Learner(n_arms = n_prices, prices=prices, tau=int(np.sqrt(365)))\n",
    "\n",
    "\n",
    "  ts_collected_costs = np.array([])\n",
    "  ucb_collected_costs = np.array([])\n",
    "  ts_daily_pricing_reward = np.array([])\n",
    "  ucb_daily_pricing_reward = np.array([])\n",
    "\n",
    "  for t in range(0,T):  # cycle on time horizon\n",
    "\n",
    "    # TS\n",
    "    pulled_bid = np.argmax(clicks(bids)-costs(bids))\n",
    "    reward_click, reward_cost = env.bidding_round(pulled_bid)\n",
    "    ts_collected_costs = np.append(ts_collected_costs, reward_cost)\n",
    "    for k in range(reward_click):\n",
    "      pulled_price = ts_learner.pull_arm()\n",
    "      reward_price = env.pricing_round(pulled_price, t)\n",
    "      ts_learner.update(pulled_price, reward_price)\n",
    "    ts_daily_pricing_reward = np.append(ts_daily_pricing_reward,(reward_click>0)*np.sum(ts_learner.collected_rewards[-reward_click:]))\n",
    "\n",
    "\n",
    "    # UCB\n",
    "    pulled_bid = np.argmax(clicks(bids)-costs(bids))\n",
    "    reward_click, reward_cost = env.bidding_round(pulled_bid)\n",
    "    ucb_collected_costs = np.append(ucb_collected_costs, reward_cost)\n",
    "    for k in range(reward_click):\n",
    "      pulled_price = learner.pull_arm()\n",
    "      reward_price = env.pricing_round(pulled_price, t)\n",
    "      learner.update(pulled_price, reward_price)\n",
    "    ucb_daily_pricing_reward = np.append(ucb_daily_pricing_reward,(reward_click>0)*np.sum(learner.collected_rewards[-reward_click:]))\n",
    "\n",
    "\n",
    "  ts_rewards_per_experiment.append(ts_daily_pricing_reward - ts_collected_costs)\n",
    "  ucb_rewards_per_experiment.append(ucb_daily_pricing_reward - ucb_collected_costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0)\n",
    "plt.ylabel(\"Regret\")\n",
    "plt.xlabel(\"t\")\n",
    "plt.plot(np.cumsum(np.mean(opt - ts_rewards_per_experiment, axis = 0)), 'r')\n",
    "plt.plot(np.cumsum(np.mean(opt - ucb_rewards_per_experiment, axis = 0)), 'b')\n",
    "plt.legend([\"TS\", \"UCB\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
