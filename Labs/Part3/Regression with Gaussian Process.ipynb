{"cells":[{"cell_type":"markdown","metadata":{"id":"LQ1WYINkTR3C"},"source":["A Gaussian process is a collection of random variables, any finite number of which have a joint Gaussian distribution.\n","Gaussian process regressors are powerful and flexible methods that exploit the correlation between input space points. They provide us not only the mean, but also the uncertainty of the estimation, which in an online problem can help in adressing the exploration-exploitation dilemma.\n","A Gaussian process is completely specified by its mean and covariance function:\n","\n","$ f(x) \\sim GP(m(x),k(x,x')) $ \n","\n","If we have not any prior information about the mean $ m(x) $, we will take it to be zero. The covatiance is given by the kernel function $ E[(f(x)-m(x))(f(x')-m(x'))] = k(x,x') $. The kernel function provides a measure of similarity between two points (how much the points influence each other in the function value estimation). The kernel function needs to be positive-definite and symmetric. A common choice is the \"squared exponential kernel\":\n","\n","$ k(x,x') = \\theta^2 e^{-\\frac{(x-x')^2}{2l^2}} $\n","\n","where $\\theta^2$ is a scale factor and $l$ is the lengthscale (controls the \"wiggliness\" of the function, tells how close two points have to be to influence each other significantly). The intuition behind this kernel is that variables that are close in the input space are highly correlated, while those far away are uncorrelated. For a single test point $x_*$, the mean and variance prediction is given by:\n","\n","$ \\mu_* = k_*^T[K_N + \\sigma_n^2I]^{-1}y $\n","\n","$ \\sigma_*^2 = k(x_*,x_*)- k_{*N}^T[K_N + \\sigma_n^2I]^{-1}k_* $ \n","\n","where $k_*$ is the covariance vector between the test point and training points, $K_N$ the covarariance matrix, $\\sigma_n^2$ the noise variance and $y$ the targets vector."]},{"cell_type":"markdown","metadata":{"id":"a7CerfXeWvZI"},"source":["We want to estimate a function that maps the number of bids (x) to the corresponding expected number of clicks (y).\n","\n","We define a set of possible bids $ X = {0.10,0.15,...,1.00} $ and a function generating the number of clicks $ n(x) = 100(1-e^{-5x}) $. We then add a Gaussian noise. We will generate samples one by one to show how the GP reduces the uncertainty of its estimation once it collects a new sample."]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":350,"status":"ok","timestamp":1682593556903,"user":{"displayName":"Manuel Piliego","userId":"14926858394514943153"},"user_tz":-120},"id":"JnC8oeC1TOrH"},"outputs":[],"source":["import numpy as np\n","from matplotlib import pyplot as plt\n","from sklearn.gaussian_process import GaussianProcessRegressor\n","from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C"]},{"cell_type":"code","source":["import sklearn as sk"],"metadata":{"id":"75Xb468_ZOcF","executionInfo":{"status":"ok","timestamp":1682593610724,"user_tz":-120,"elapsed":4,"user":{"displayName":"Manuel Piliego","userId":"14926858394514943153"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1682591437071,"user":{"displayName":"Manuel Piliego","userId":"14926858394514943153"},"user_tz":-120},"id":"BGRYpZZ9XxzG"},"outputs":[],"source":["# function that given a bid, returns the expected number of clicks\n","def n(x):\n","  # the real function to estimate\n","  return (1.0 - np.exp(-5.0*x)) * 100"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1682591437072,"user":{"displayName":"Manuel Piliego","userId":"14926858394514943153"},"user_tz":-120},"id":"8rXCFuuHYAdA"},"outputs":[],"source":["# function that generates the observation by adding some noise\n","def generate_observations(x, noise_std):\n","  return n(x) + np.random.normal(0,noise_std, size = n(x).shape)"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1MZKpxr7dTbjdrRwf3Y0Loa0VtJ8wDICZ"},"executionInfo":{"elapsed":22999,"status":"ok","timestamp":1682593786412,"user":{"displayName":"Manuel Piliego","userId":"14926858394514943153"},"user_tz":-120},"id":"8AChTiGBYPxV","outputId":"b21cc5a2-6480-425d-f2c8-74b4e6f8128d"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["n_obs = 50\n","bids = np.linspace(0.0,1.0,20)\n","x_obs = np.array([])\n","y_obs = np.array([])\n","noise_std = 5.0\n","\n","# loop in which at each iteration we generate a new observation, fit a GP and plot the function estimated by the GP\n","for i in range(0,n_obs):\n","  new_x_obs = np.random.choice(bids,1)\n","  new_y_obs = generate_observations(new_x_obs,noise_std)\n","\n","  x_obs = np.append(x_obs,new_x_obs)\n","  y_obs = np.append(y_obs, new_y_obs)\n","\n","  # initialize GP and set its parameters\n","  X = np.atleast_2d(x_obs).T\n","  Y = y_obs.ravel()\n","\n","  theta = 1.0\n","  l = 1.0\n","  # kernel: product of constant kernel and RBF kernel. To both we pass the value of their variable and its possible range\n","  kernel = C(theta, (1e-3,1e3)) * RBF(l,(1e-3,1e3))\n","  # intialize the GP by passing the kernel, the noise variance and flagging the y normalization\n","  gp = GaussianProcessRegressor(kernel=kernel, alpha=noise_std**2,  n_restarts_optimizer = 10)  # normalize_y=True, gives some issues\n","  # when we fit the GP, we will find the optimum value for the hyperparameters by maximizing the marginal likelyhood; this operation is done \"n_restarts_optimizer\" times starting from random points\n","  \n","  gp.fit(X,Y)\n","\n","  # prediction and uncertainty plot\n","  x_pred = np.atleast_2d(bids).T\n","  y_pred, sigma = gp.predict(x_pred, return_std = True)\n","\n","  plt.figure(i)\n","  plt.plot(x_pred, n(x_pred), 'r:', label = r'$n(x)$')\n","  plt.plot(X.ravel(), Y, 'ro', label = u'Observed Clicks')\n","  plt.plot(x_pred, y_pred, 'b-', label = u'Predicted Clicks')\n","  plt.fill(np.concatenate([x_pred,x_pred[::-1]]),\n","           np.concatenate([y_pred - 1.96 * sigma , (y_pred + 1.96 * sigma)[::-1]]),\n","           alpha = .5, fc = 'b', ec = 'None', label = '95% conf interval')\n","  plt.xlabel('$x')\n","  plt.ylabel('$n(x)$')\n","  plt.legend(loc = 'lower right')\n","  plt.show()"]},{"cell_type":"markdown","metadata":{"id":"O2AISLutZTyu"},"source":["A good practice with GP is to normalize the data (in our case its not needed on the x and it is built in for the y). Then we need to specify the kernel function, set the kernel parameters (for the squared exponential kernel a common choice is lengthscale $l = 1$ and scale factor (variance) $ \\theta = 1 $ ). The the GP is trained and, if necessary, the hyperparameters are estimated from the data by maximizing the marginal likelihood or with other techinques.\n","\n","As points are collected, the GP approaches to the real function and the uncertainty is reduced. The uncertainity is bigger where there are less observations."]},{"cell_type":"code","source":["GaussianProcessRegressor.__version__"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":171},"id":"QBU5U4exYZjC","executionInfo":{"status":"error","timestamp":1682593565977,"user_tz":-120,"elapsed":4,"user":{"displayName":"Manuel Piliego","userId":"14926858394514943153"}},"outputId":"683ff6d0-3d70-47e2-bdf8-36d925771bc5"},"execution_count":23,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-3a76e28a141e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mGaussianProcessRegressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mAttributeError\u001b[0m: type object 'GaussianProcessRegressor' has no attribute '__version__'"]}]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPMDeIvGriXkHaF9Pz5GTFW"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}